---
title: "Rocker"
author: ""
output:
  html_document:
    keep_md: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

I'm going to create a container that creates an R Shiny Map that is then served up via a container.  If you like R and are working with Docker and/or Kubernetes, then [Rocker](https://www.rocker-project.org/) is for you!  While this particular project is uses some specialized geospatial data techniques, the pattern solves the common problem of integrating data science work with the larger tech stack using standard methods.

The aim of the geospatial R project is to determine the quality of the environment for a given location .  Lucky for us, [I've done some previous work in this area for the Illinois EPA](https://github.com/TroyHernandez/IL_EJ_map/).

Briefly, the US EPA provides a data set containing several health-related measures; e.g. `National Scale Air Toxics Assessment Air Toxics Cancer Risk`, `Particulate Matter (PM2.5)`, `Lead Paint Indicator`, `Traffic Proximity and Volume`, etc.  There is an existing aggregate of those measures called the [CalEnviroScreen](https://oehha.ca.gov/calenviroscreen) used by the state of California to determine [environmental justice communities](https://en.wikipedia.org/wiki/Environmental_justice) in the state.  For the IL EPA, I was helping them determine which census tracts qualified under the new [Future Energy Jobs Act](https://www.illinois.gov/sites/ipa/Pages/Renewable_Resources.aspx), which was modeled after CalEnviroScreen.

For this project, I'll be calculating that derived metric for, not just California or Illinois, but for the entire country! Additionally, I'll have to use a person's location and the shapefiles of every census tract in the country to determine their exposure to pollution.

This document takes you through the process of:

1. Pulling the Rocker image
2. Doing the actual GIS work in R
3. Modifying a container from that image to do some GIS work in R
4. Creating a Helm chart from the Dockerfile to deploy the image within the cluster
5. Deploying the Helm chart

## Pulling the Rocker image

If you're not familiar with the [Rocker](https://www.rocker-project.org/) project, click through.

First you need to pull the image from Docker (R 4.0 isn't stable for me yet):


```{bash}
sudo docker pull rocker/shiny-verse:3.6.3
```

We’ll start by running the container in docker as specified on the Rocker page, but with a proper name.

```{bash}
docker run -d --name geo1 -p 3838:3838 \
    -v /srv/shinyapps/:/srv/shiny-server/ \
    -v /srv/shinylog/:/var/log/shiny-server/ \
    rocker/shiny-verse:3.6.3
```

There you go! You just created a container that serves up an R Shiny app!

## Doing the actual GIS work in R

Before we do more work on our image and in a container, let's shift gears and get the actual data science work done first.  That will allow us to know exactly what we need baked into our image.

To summarize again, we will be taking a data set from the US EPA consisting of several health measures per census tract.  Then we will aggregate those measures into a single metric as per the CalEnviroScreen specifications.  Then we match that metric to shapefiles describing the boundaries of each census tract.

### Preliminaries

We'll need the `sf` and `tigris` packages to work with and download shapefiles.

```{r}
install.packages('sf')
install.packages("tigris")
```

We'll also download the EJscreen data from the EPA.

```{r}
download.file("ftp://newftp.epa.gov/EJSCREEN/2018/EJSCREEN_2018_USPR_csv.zip", "data/EJSCREEN_2018_USPR_csv.zip")
unzip("data/EJSCREEN_2018_USPR_csv.zip", exdir = "data/")
```

Finally, we download each state's census tract shapefiles using the `tigris` package and their [FIPS](https://en.wikipedia.org/wiki/Federal_Information_Processing_Standards) id.

```{r}
fips <- read.csv("https://raw.githubusercontent.com/kjhealy/fips-codes/master/state_fips_master.csv")
for(i in 1:50){
  saveRDS(tigris::tracts(state = fips$fips[i], class = "sf"),
          file = paste0("data/censustracts/", fips$state_abbr[i], ".Rds"))
}

# also Washington DC
saveRDS(tigris::tracts(11, class = "sf"),
        file = paste0("data/censustracts/", "DC", ".Rds"))
```

### Build environmental metric data

Now we start building the metric.  First we get the `geoid` for each census tract from our shapefiles.

```{r}
lf <- list.files("data/censustracts")
geoid <- unlist(lapply(lf, FUN = function(x){readRDS(paste0("data/censustracts/", x))$GEOID}))
```

Then we load in the EPA dataset and ensure that our metrics are numeric.

```{r}
dat <- read.csv("data/EJSCREEN_Full_USPR_2018.csv", stringsAsFactors = FALSE)
dat.env <- dat[, c("CANCER", "RESP", "DSLPM", "PM25", "OZONE", "PRE1960",
                   "PTRAF", "PRMP", "PTSDF", "PNPL", "PWDIS")]
for(i in 1:ncol(dat.env)){
  temp <- as.numeric(dat.env[, i])
  temp[is.na(temp)] <- 0
  dat.env[, i] <- temp
}
```

I'm using this dataset for more environmental justice projects, so I'm going to calculate the demographic variables and the final "EnviroScore" for the CalEnviroScreen while I'm at it.  I'm fairly certain that using many of these demographics would be very illegal in the financial services sector, so don't do that.

```{r}
dat.demo <- dat[, c("LOWINCPCT", "MINORPCT", "LESSHSPCT", "LINGISOPCT",
                    "UNDER5PCT", "OVER64PCT")]

dat2 <- cbind(dat.env, dat.demo)
```

A handful of tracts have multiple rows in the EPA dataset.  We take the average of each tracts rows to ensure that we get a unique metric for each tract.  Then we check to make sure that we have the same geoids in both datasets. (With ~73k rows, this takes a few minutes to run.  There are faster ways to do this, but I'm currently in no hurry.)

```{r}
dat3 <- aggregate(dat2, by = list(ID.tract), FUN = "mean")
colnames(dat3)[1] <- "ID"
sum(as.character(dat3$ID)!=sort(geoid))
# [1] 0
```

The CalEnviroScreen aggregated metric takes the percentile (relative to every other census tract) of each environmental (and demographic) metric and then averages those scores to obtain a cumulative environmental (demographic) metric.  The final CalEnviroScore (not used here) is obtained by multiplying those two metrics.

```{r}
perc.rank <- function(x) trunc(rank(x)) / length(x)
dat4 <- data.frame(ID = as.character(dat3[, 1]),
                   apply(dat3[, -1], 2, perc.rank))
rownames(dat4) <- as.character(dat4$ID)
dat5 <- dat4[geoid, ]
dat.env2 <- rowMeans(dat5[, c("CANCER", "RESP", "DSLPM", "PM25", "OZONE",
                              "PRE1960", "PTRAF", "PRMP", "PTSDF", "PNPL",
                              "PWDIS")])
dat.demo2 <- rowMeans(dat5[, c("LOWINCPCT", "MINORPCT", "LESSHSPCT",
                               "LINGISOPCT", "UNDER5PCT", "OVER64PCT")])
dat.EnviroScore <- (dat.env2 * dat.demo2)
```


Then we add back in the original data for future reference and finally save that in an Rds file; the R binary data format.

```{r}
dat6 <- data.frame(ID = as.character(dat3[, 1]),
                   dat3[, -1])
rownames(dat6) <- as.character(dat6$ID)
dat7 <- dat6[geoid, ]

dat.shiny <- data.frame(CensusTract = geoid,
                        # EJcommunity = dat.EJ,
                        EnviroScore = dat.EnviroScore,
                        EnvironmentalIndicator = dat.env2,
                        DemographicIndicator = dat.demo2,
                        dat7[, -1])
row.names(dat.shiny) <- NULL
saveRDS(dat.shiny, "data/ShinyDat_USA.RDS")
```

### Writing the Shiny file

The team came to the agreement that they would deliver to me a latitude, longitude, and state and I would return to them the environmental score as a proxy for insurability.  So now we write that function/file.  This is that file:

```{r}
# Shiny_geo.R

library(sf)
dat.shiny <- readRDS("/home/CHI_EJ_map/data/ShinyDat_USA.RDS")

#* @post /enviro
EnviroInd <- function(lat, lon, state){
  lat <- as.numeric(lat)
  lon <- as.numeric(lon)
  state <- as.character(state)

  State <- readRDS(paste0("/home/CHI_EJ_map/data/censustracts/", state, ".Rds"))
  state_tracts <- State[, c("GEOID", "TRACTCE")]
  
  # convert the points to same CRS
  my_points_tract <- sf::st_join(sf::st_as_sf(data.frame(x = lon, y = lat) , coords = c("x", "y"),
                                              crs = sf::st_crs(state_tracts)),
                                 state_tracts)$GEOID
  dat.shiny[my_points_tract, "EnvironmentalIndicator"]
}
```

You'll notice that the global parameters are at the top of this file that we've named `Shiny_geo.R`.  Then there is a regular ol' R function.  The only difference is the line consisting of `#* @post /enviro`.  Those are instructions for the Shiny API.  I won't go into details here, but there is great documentation [here](https://www.rShiny.io/docs).  Additionally, there are plenty of great resources (read: StackOverflow) to get you acquainted with the sf package.

You can test this code on your own machine.  If you don't want to copy and paste everything, you can find all necessary files at [my github repo](https://github.com/TroyHernandez/CHI_EJ_map/).

Now we have to take everything we just did and put it into a container!

## Modifying a container from that image to do some GIS work in R

The Rocker project has [a pre-built container for doing geospatial work](https://github.com/rocker-org/geospatial)... and we just played with the Shiny container... But we need a Shiny container that also does geospatial work!

Containers are meant to be lightweight so we'll have to the add packages and libraries that we need.  Moreover, we don't need a Shiny container that does just any ol' geospatial work.  We need one that does *our* geospatial work.  So we'll have to add some of our own code.

There are two ways to go about doing this:

1. We can `docker exec` into our Shiny container's terminal and add things from the command line, or
2. We can write a `dockerfile` that will build the container to our specifications.

It's hard to know a priori what needs to be added into Shiny container. So we'll use the first method to explore what needs to be added and to make sure everything we add works corrrectly.  Then we'll take what we learned and harden it into a dockerfile.  For the data scientists out there, the process is not dissimilar to doing an exploratory data analysis followed by writing a package once you've got an idea of what is going on.

### `docker exec`

To add the geospatial libraries and packages, we need to get into the container and get access to its command line.  When working with remote servers, we usually turn to `ssh`.  For docker containers, we use the `docker exec` command.

```{bash}
sudo docker --help | grep "exec"
#  exec        Run a command in a running container
```

So we first exec into our Shiny container, which should give you a prompt like the one here:

```{bash}
sudo docker exec -it $(sudo docker ps -q --filter "name=geo1") bash
root@9032aa31a561:/#
```

Then we install git, pull my git repo into that container's home folder, run the R script, and see if it works!

```{bash}
apt-get update
apt-get install git-core
git clone https://github.com/troyhernandez/CHI_EJ_map /srv/shinyapps/CHI_EJ_map
cd /srv/shinyapps/CHI_EJ_map
R
```

Now that we're in our R prompt (in our container), we can see if our stuff works! First let's try loading the neccessary `sf` package.

```{r}
library(sf)
# Error in library(sf) : there is no package called ‘sf’
```

Sadly, there is no R package called `sf` installed in this container... not that we would expect there to be.  So let's install it!

```{r}
install.packages("sf")
# ...
# ERROR: dependency ‘units’ is not available for package ‘sf’
```

You'll see that this fails with the most helpful error being the included comment. Some searching will inform you that we need to install some libraries into the container before we'll be successful with the `sf` package.  So let's quit (`q()`) the R interpretor and get those installed with:

```{bash}
apt-get update
apt-get install libudunits2-dev  libgdal-dev
<!-- apt-get install libgdal20 -->
```

You'll find that the `sf` package now installs without incident (as long as you're not behind a vpn/proxy, which seems to mess up installing libraries into a docker container). Pull up the R command prompt again and try the `install.packages("sf")` command again. Success!

Feel free to play around with the `EnviroInd()` function/code if you're so interested, but more importantly we need to ensure that our new container serves up that API!

#### Saving that container to a new image

To ensure that the libraries and packages we loaded into the docker container persists, we need to save the container down to a new image.  `exit` out of your container and run this to save a new image:


```{bash}
sudo docker commit -m "Fixed it!" -a "Troy Hernandez" geo1 shiny/geo:v2
```

#### Testing the Shiny/API functionality of that image

Now we can stop our geo1 container and run our new geo:v2 container.

```{bash}
docker run -d --name geo2 -p 3838:3838 \
    -v /srv/shinyapps/:/srv/shiny-server/ \
    -v /srv/shinylog/:/var/log/shiny-server/ \
    shiny/geo:v2

sudo docker run -d --name geo2 -p 3838:3838 Shiny/geo:v2
```

As above, go to a different terminal and test the api.  I used the latest data point we had to confirm everything was working... it did for me!

```{bash}
curl -H "Content-Type: application/json" --data '{"lat": 43.8138, "lon": -91.2519, "state": "WI"}' localhost:3838/enviro
# [0.5447]
```

### Creating a Dockerfile to recreate the aforementioned image (Reproducible research FTW!)

While we were successful in creating the neccessary image above, it's helpful to bake that all into a `Dockerfile`.  Dockerfiles make your image reproducible and explicitly state what is included.  We take the [trestletech Shiny Dockerfile](https://github.com/trestletech/Shiny/blob/master/Dockerfile) and add a few things to create the above image:

```
FROM trestletech/Shiny
MAINTAINER Troy Hernandez <troy.hernandez@ibm.com>

RUN apt-get update -qq && apt-get install -y \
  git-core \
  libssl-dev \
  libcurl4-gnutls-dev \
  nano \
  libudunits2-dev \
  libgdal20 \
  libgdal-dev

RUN R -e 'install.packages(c("sf"))'
RUN install2.r Shiny
RUN git clone https://github.com/troyhernandez/CHI_EJ_map /home/CHI_EJ_map

EXPOSE 3838
ENTRYPOINT ["R", "-e", "pr <- Shiny::plumb(commandArgs()[4]); pr$run(host='0.0.0.0', port=3838)"]
CMD ["/home/CHI_EJ_map/R/Shiny_geo.R"]
```

```
docker build –t shiny/geo:v1 .
docker build --tag shiny/geo:1.0 .
```

## Posting Dockerfile to a container registry

### Login to IBM Cloud

Because I'm an IBM employee I have to do this:

```{bash}
ibmcloud login --sso
```

Civilians should follow this:

https://cloud.ibm.com/kubernetes/registry/main/start

```{bash}
ibmcloud cr image-list
```

## Creating an operator from a Dockerfile to deploy the image within the cluster

```{bash}
oc login --token=5NzSmqzQr-aGn_iUO3TdASv4QabahuCZAv9eO91FzI8 --server=https://c106-e.us-south.containers.cloud.ibm.com:30313
```

## Deploying the Helm chart
